# ===============================================
# AZURE FABRIC / ONELAKE CONFIGURATION
# ===============================================
# Configures Fabric workspace, lakehouses, and OneLake paths
# for the Altaviz production pipeline (4,700 compressors)
#
# OneLake uses ABFS protocol:
# abfss://<workspace>@onelake.dfs.fabric.microsoft.com/<lakehouse>/

# Fabric Workspace
workspace:
  name: "altaviz-production"
  id: "${FABRIC_WORKSPACE_ID}"
  capacity: "F64"    # 64 CU - handles 4,700 compressors
  region: "South Central US"    # Co-located with Archrock HQ (Houston)

# Lakehouse Definitions (one per medallion layer)
lakehouses:
  bronze:
    name: "lh_bronze_raw"
    id: "${FABRIC_BRONZE_LAKEHOUSE_ID}"
    description: "Immutable raw sensor telemetry from IoT Hub"
    retention_days: 2555    # 7 years (EPA compliance)
    path: "abfss://altaviz-production@onelake.dfs.fabric.microsoft.com/lh_bronze_raw"

  silver:
    name: "lh_silver_cleaned"
    id: "${FABRIC_SILVER_LAKEHOUSE_ID}"
    description: "Cleaned, validated, deduplicated sensor data"
    retention_days: 365
    path: "abfss://altaviz-production@onelake.dfs.fabric.microsoft.com/lh_silver_cleaned"

  gold:
    name: "lh_gold_curated"
    id: "${FABRIC_GOLD_LAKEHOUSE_ID}"
    description: "Aggregated metrics, ML features, business-ready"
    retention_days: 365
    path: "abfss://altaviz-production@onelake.dfs.fabric.microsoft.com/lh_gold_curated"

  ml:
    name: "lh_ml_serving"
    id: "${FABRIC_ML_LAKEHOUSE_ID}"
    description: "ML predictions, feature store, model artifacts"
    retention_days: 90
    path: "abfss://altaviz-production@onelake.dfs.fabric.microsoft.com/lh_ml_serving"

# OneLake Table Paths (Delta format)
tables:
  # Bronze layer tables
  bronze:
    sensor_readings: "Tables/sensor_readings"
    compressor_metadata: "Tables/compressor_metadata"
    maintenance_events: "Tables/maintenance_events"
    raw_events: "Files/raw_events/"    # Landing zone for Event Hubs

  # Silver layer tables
  silver:
    sensor_cleaned: "Tables/sensor_cleaned"
    quality_metrics: "Tables/quality_metrics"

  # Gold layer tables
  gold:
    hourly_aggregates: "Tables/hourly_aggregates"
    daily_aggregates: "Tables/daily_aggregates"
    alert_history: "Tables/alert_history"
    fleet_health: "Tables/fleet_health"
    emissions_estimates: "Tables/emissions_estimates"

  # ML layer tables
  ml:
    predictions: "Tables/ml_predictions"
    features: "Tables/feature_store"
    model_metrics: "Tables/model_metrics"

# Partitioning Strategy
partitioning:
  bronze:
    partition_by: ["ingestion_date"]
    file_format: "delta"
  silver:
    partition_by: ["date"]
    file_format: "delta"
  gold:
    partition_by: ["date", "region"]
    z_order_by: ["compressor_id"]
    file_format: "delta"

# Spark Configuration (Fabric runtime)
spark:
  app_name: "AltavizProductionETL"
  runtime: "1.3"    # Fabric Spark runtime (Spark 3.5, Delta 3.1)
  configs:
    # Fabric-specific
    "spark.fabric.workspace.id": "${FABRIC_WORKSPACE_ID}"
    "spark.fabric.lakehouse.default": "lh_gold_curated"

    # Performance tuning for 4,700 compressors
    "spark.sql.adaptive.enabled": "true"
    "spark.sql.adaptive.coalescePartitions.enabled": "true"
    "spark.sql.adaptive.skewJoin.enabled": "true"
    "spark.sql.shuffle.partitions": "200"

    # Delta Lake optimizations
    "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension"
    "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog"
    "spark.databricks.delta.optimizeWrite.enabled": "true"
    "spark.databricks.delta.autoCompact.enabled": "true"

    # Memory (Fabric manages allocation, but hints help)
    "spark.driver.memory": "16g"
    "spark.executor.memory": "16g"
    "spark.executor.cores": "4"
    "spark.executor.instances": "8"

    # Arrow for Pandas interop
    "spark.sql.execution.arrow.pyspark.enabled": "true"

# Event Hubs Configuration (IoT ingestion)
event_hubs:
  namespace: "${EVENTHUB_NAMESPACE}"
  hub_name: "compressor-telemetry"
  consumer_group: "etl-pipeline"
  connection_string: "${EVENTHUB_CONNECTION_STRING}"
  max_events_per_trigger: 100000
  starting_position: "latest"
  partitions: 16    # 4,700 compressors / ~300 per partition

# Data Quality
data_quality:
  max_missing_rate: 0.05
  outlier_std_threshold: 4.0
  freshness_sla_minutes: 15
  min_compressors_per_batch: 4000    # Alert if below 85% of fleet

# Processing Schedule
schedule:
  streaming:
    trigger_interval: "5 minutes"
    watermark_delay: "4 hours"
    checkpoint_location: "Files/checkpoints/streaming/"
  batch:
    bronze_to_silver: "*/15 * * * *"    # Every 15 minutes
    silver_to_gold: "0 * * * *"          # Every hour
    ml_inference: "0 */4 * * *"          # Every 4 hours
    data_archival: "0 2 * * *"           # Daily at 2 AM

# Monitoring
monitoring:
  pipeline_runs_table: "Tables/pipeline_runs"
  alert_webhook: "${TEAMS_WEBHOOK_URL}"
  metrics_namespace: "altaviz.pipeline"
  log_analytics_workspace: "${LOG_ANALYTICS_WORKSPACE_ID}"
