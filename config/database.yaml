# ===============================================
# DATABASE CONFIGURATION
# ===============================================
# Configuration for database and Delta Lake connections
# Uses PostgreSQL (Supabase free tier) with connection string auth
# Environment variables are substituted using ${VAR_NAME:-default_value} syntax

# -----------------------------------------------
# DATABASE TYPE SELECTOR
# -----------------------------------------------
db_type: "postgresql"

# -----------------------------------------------
# POSTGRESQL SETTINGS (Supabase)
# -----------------------------------------------
postgresql:
  connection_url: "${DATABASE_URL}"
  schema: "public"
  # JDBC settings for PySpark writes
  jdbc_url_template: "jdbc:postgresql://{host}:{port}/{database}"
  jdbc_driver: "org.postgresql.Driver"

# -----------------------------------------------
# CONNECTION POOL SETTINGS
# -----------------------------------------------
pool:
  min_size: 2
  max_size: 10
  timeout: 30

# -----------------------------------------------
# DELTA LAKE CONFIGURATION
# -----------------------------------------------
delta_lake:
  checkpoint_location: "data/processed/checkpoints/"
  log_retention_duration: "interval 30 days"
  deleted_file_retention_duration: "interval 7 days"

# -----------------------------------------------
# NOTES
# -----------------------------------------------
# - This file is loaded by src/etl/utils.py
# - Environment variables are read from .env file in project root
# - Never commit database credentials to version control
# - Auth uses DATABASE_URL connection string
# - For local dev: use .env file (gitignored)
